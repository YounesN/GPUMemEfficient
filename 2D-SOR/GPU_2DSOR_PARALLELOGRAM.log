Version 1.0: 
1. In the processing of last batch of tile, "else if (curBatch == yseg - 1)", we made several assumption to make the development simple. Search "version 1.0" to see the comments of these assumptions in the code file.
2. "dep_stride = stride + 1" is correct based on the assumption that "stride == 1". When "stride is larger than 1, 
it is possible that "dep_stride = 2 * stride"


Completed:
1. skelenton code is completed.
2. stream scheduling, kernel launching, and lock functions are working properly.
3.

################################################################
May 15 2019
################################################################
Solved:
1. if (curBatch == 0): moveMatrixToTile() function is now working properly when copying a tile of size 4*4 to shared memory.
2. if (curBatch == 0): moveIntraDepToTileEdge() function works properly when tt = 0 and tileT = 1.
3. if (curBatch == 0): moveInterDepToTileEdge() function works properly when tt = 0 and tileT = 1.


Problem:
1. register resources exceed the capacity when threadsPerBlock = 1024.
2. SOR.cpp: incorrect readInputData() function. Cannot skip the first row of data. Either fix it or change paddsize to 1.


###############################################################
Jun 5th
###############################################################
Solved:
1. SOR.cpp: readInputData() function. Read all data and process only the central block of (n1+2) * (n2+2).


###############################################################
Jun 10th
##############################################################
Problem:
1. moveTileToInterDepEdge() function does not access correct memory address. The print out message is not right.
Solved;


##############################################################
Jun 11th
#############################################################
Problem:
1. moveTileToInterDep() function: inter_stream_dep[] is not stored correctly, the print information is not correct.
Found not solved.

##############################################################
Jun 17th
#############################################################
Problem:
1. in function moveMatrixToTile(), the element for the last row of the tile are not stored correctly into the shared memory.
Data entries could be accessed correctly; however, some data entries cannot be properly stored into the shared memory.
This problem is caused by swaping the data entries between shared memory tile1 and tile2.
Solved: Introducing threadfence() before the swap function.


###########################################################
Jun 18th
###########################################################
Solved:
1. "width" and "height" variables are revised. 
2. curBatch == 0, tileIdx >= 1 && tileIdx < xseg - 1; "newtilePos" is changed to store the results into the tile area instead of dependent data area.
3. curBatch == 0, tileIdx >= 1 && tileIdx < xseg - 1; "glbPos" is changed from "glbPos = tileAddress" to "glbPos = tileAddress - tileT" to adapt the shift along x dimension.
4. In function moveShareToGLobal() and moveShareToGlobalEdge(), we should use "width" instead of "height" as parameter.

Needs to be fixed:
1. Like what we did to "glbPos" in this work, we might need to change "glbPos" accordingly in other simulation to adapt the shift of data entries.


##########################################################
July 12
##########################################################
idea:
1. replace cudaMalloc with cudaManagedMalloc() to see if it allows a large size test case.

##########################################################
July 28th
##########################################################
Problem:
1. moveIntraDepToShare() may encounter error. The intra-dep data should be copied to the array for next tile before updating the next time step results of the current tile.
	a. Move intra-dep array data to shared memory block;
	b. Calculate the next time stamp result of the shared memory block;
	c. Move intra-dep data for next tile to intra-dep array.
	d. Move shared memory block back to GPU memory array.
Solved: Instead of copy data entries from global memory to intra_dep array, in shared memory, we copy the intra_dep data that locates in shared memory block, which are not updated by next stamp result, to intra_dep array.

Completed:
1. first batch with tileT == 1 is completed. Test case 3 * 3 matrix with size 2 padding.


##########################################################
Aug 18th
##########################################################
Problem:
1. For "else if(curBatch = yseg - 1)", the first tile, what is the correct value for variable "len" in "moveIntraDepToTileEdge()" function. Temporary, we make "len == tt + dep_stride" which is not correct when "stride > 1".
  


##########################################################
Aug 20th
##########################################################
Change:
1. Function "moveIntraDepToTileEdge()", pointer variable "tile" is changed from "&tile1[0]" to "&tile1[x]", where "x" is the start address set up accordingly.
2. Function "moveIntraDepToTileEdge()", pointer variable "dev_arr" is changed from "dev_arr" to "&dev_arr[x]", where "x" is the start address set up accordingly.
3. In MID_BATCH branch, the first tile calls "moveIntraDepToTileEdge()" two times. One for the elements in prior to the tile entries; the other one for the elements in prior to the inter_dep entries.
4. In MID_BATCH branch, the first tile: before function "moveShareToGlobalEdge()", variable "glbPos" is changed from 
tileAddress" to "tileAddress - tileT * width".
5. Function "moveInterDepToTileEdge()", pointer variable "dev_arr" is changed from "dev_arr" to "&dev_arr[x]", where "x" is the start address set up accordingly.


#########################################################
Aug 22st
#########################################################
Change:
1. In memory copy functions like "moveInterDepToTileEdge()", "moveIntraDepToTileEdge()", and etc, the shifting offset is considered and applied in two places:
	a. function passing parameter: pass the pointer, pointed to an adjusted address.
	b. inside of each function, add shifting adjustment, which is decided by timestamp "tt".

To do:
1. In function "moveInterDepToTileEdge()", the if statement "if (threadIdx.x < len + dep_stride)" might not be correct. Not sure if "dep_stride" should be removed because it is already included in "len". Also, we only copy the out-of-range entries in this function and do not need to include the "dep_stride" number of inter-dep entries, which are in prior to the out-of-range inter-dep entries (duplicate copy).

2. We now complete the testing for tileT == 1, 2, 3. However, the MAXTRIAL is still 1. Next step, we should test the case that tileT > 1 and MAXTRIAL > 1, which requires lock synchronize and global memory coherence.


#########################################################
Aug 27th
#########################################################
Problem:
1. inter_dep array is updated by the later calculation before it is used for the current calculation. This happens at the beginning of a new timepiece and the number of streams is limited.
For Example: 2 streams (s0, s1), 3 batches.
batch:		1	2	3
timepiece 1:   s0      s1      s0
timepiece 2:   s1      s0      s1

When s0 is calculating on batch 3 at timepiece 1, it is possible that s1 already starts working on batch 1 timepiece 2 concurrently. In this case, it is possible that s1 is in prior to s0, thus s1 would update the new calculated inter-dep data to s0's inter-dep array, which overwrites the current data and breaks the s0's calculation.

Possible solution: In each timepiece, the first batch can only start when the next stream already complete the calculation.
For example:  4 streams, 5 batches
batch:		1	2	3	4	5
timepiece 1:   s0	s1	s2	s3	s0
timepiece 2:   s1	s2	s3	s4	s0

In this example, s1 can only start working on batch 1 timepiece 2 when s2 already completes its work on batch 3 timepiece 1.
It is also possible that this check has to be performed at the beginning of each batch, not only limited to the first batch.

#######################################################
Sep 1st
#######################################################
Completion:
1. "dev_time_lock" and the related lock functions are designed and working properly.















































